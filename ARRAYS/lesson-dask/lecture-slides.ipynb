{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b272c6c-1c52-4c71-bf33-00072b264b08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lesson 3: Vertical and horizontal scaling\n",
    "\n",
    "(all credit to J. Pivarski for jupyter notebooks and images, Histogramming tools by L. Gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201753d-9878-4eda-8f06-cd01d39046ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This lesson is about making Python faster, in two dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a7519-5e33-4422-94fd-b43a1bf62b07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"../img/horizontal-and-vertical-scaling.svg\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797adfb6-b459-4473-a3fd-b9fc6ff2f8dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vertical scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71103f6-8557-4ac4-8ccd-bbecbebfab15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Reminder: Python is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a16a0-2a19-4d55-ac94-3fa423a31803",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"../img/benchmark-games-2023.svg\" width=\"90%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe3feb-af5f-4434-8fa9-a9b187d9c57a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We have already seen that NumPy (and Awkward Array) can circumvent Python's slowness by doing computationally intensive work in compiled code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908f9d1-a7e9-4ab6-8ec1-923b3e63ed97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "events = ak.from_parquet(\"../data/SMHiggsToZZTo4L.parquet\")[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5db5a-e5a1-418e-9263-e2ae294fcd70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23856675-6233-473d-986f-a659baaeead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(5, 10, 1000000)\n",
    "b = np.random.uniform(10, 20, 1000000)\n",
    "c = np.random.uniform(-0.1, 0.1, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3da69-2206-44ac-aad5-2d4f6bf1032b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "\n",
    "pz = [[muon.pt * np.sinh(muon.eta) for muon in event.muon] for event in events]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea991f-8251-4c18-834b-325aafc8229a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2b6c6-8957-400a-8971-5cad9937e7c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "\n",
    "pz = events.muon.pt * np.sinh(events.muon.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901d264-a1c0-49f0-ac1e-43a12a805c70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Array-oriented programming connects Python with compiled code, but it's not the only way to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c2d25-a43d-4665-b39e-8ca8a7625fe2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"../img/history-of-bindings-2.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6482cf-e2e1-4b8c-ac3f-5136dfb3b40e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numba as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb6e68-21e4-45a6-89ea-3de5e3661e7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bba3c-7144-402e-b287-780911b0f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_formula(a, b, c):\n",
    "    return (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cc23c-7be8-4cc1-bbf2-c11d49e230f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "output = quadratic_formula(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573a72c-a4af-4e9a-848e-0018270ead6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def quadratic_formula_numba(a, b, c):\n",
    "    output = np.empty(len(a), dtype=np.float64)\n",
    "    for i, (a_i, b_i, c_i) in enumerate(zip(a, b, c)):\n",
    "        output[i] = (-b_i + np.sqrt(b_i**2 - 4*a_i*c_i)) / (2*a_i)\n",
    "    return output\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42ef27-ef49-4ebe-9fd5-0005bbbdba35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2746b-08b1-497c-81ae-36f4a5037882",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "quadratic_formula_numba(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7990fad-f050-4c34-860a-94ecd3e4fa19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The exercises are on vertical scaling, so do one now, before I talk about horizontal scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9e81f-c6d7-47cb-86ca-5cc41b1569be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Horizontal scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cfb84-713d-4ee9-b2bc-aee37be7b5d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"../img/horizontal-and-vertical-scaling.svg\" width=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601e5b9-7883-4ba6-be1e-d7ca2bca6149",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "There are many ways to distribute a computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03791284-3a92-4adb-a73c-84f6384117d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bad849-8dfb-4981-a1b0-55b1b327b9d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The traditional way is to use a batch queue, such as the LHC GRID. You can run Python scripts in a batch queue as easily as anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca9196-401f-48d2-82d3-e5fd0b9bb044",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2aae4-6afe-44e7-9a22-1ecd0af15e1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This section will be about Dask, a popular way to distribute computations _within_ Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39b24b-5800-4c36-ae3a-d9178649567d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"../img/logo-dask.svg\" width=\"30%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f5342-4876-4c74-ab37-6f286392f23e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Dask is a library for describing a computation as a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c39768-6eed-4577-94c5-965001f7fc3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b576c-436c-414a-b552-d5b00076357d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9e581-463c-4d1e-9b4d-65c0a2183b4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Eager Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd25700-1608-4152-ab58-cd6dca26aea0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def increment(i):\n",
    "    return i + 1\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "a, b = 1, 12\n",
    "c = increment(a)\n",
    "d = increment(b)\n",
    "output = add(c, d)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c399aa-6a20-4014-a53b-591e0f9d8f76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c46e0-3b57-46c1-b660-5156f0be10b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Lazy Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d4820-8db8-4939-a326-e8214860f047",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def increment(i):\n",
    "    return i + 1\n",
    "\n",
    "@dask.delayed\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "a, b = 1, 12\n",
    "c = increment(a)\n",
    "d = increment(b)\n",
    "output = add(c, d)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48ddb4-deae-46bd-b479-78753cf99c30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6815d0d-a2aa-4837-ab76-3ca8c0299029",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee526f-18cd-4dfa-ad6a-3862977f05dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8882b83-d62d-41d6-8dd2-e35c2468222c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "a = da.random.uniform(5, 10, 1000000)\n",
    "b = da.random.uniform(10, 20, 1000000)\n",
    "c = da.random.uniform(-0.1, 0.1, 1000000)\n",
    "\n",
    "output = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675480b9-d530-4b1e-b144-e69044bd7a87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73681c81-12ba-4ec4-a68a-956b9cf9926d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877c712-99de-42e3-a0a9-47d1e31cff0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A task graph is a delayed computationâ€”all the instructions that are needed to perform the computation at a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7eb8f-3cb3-49f2-9b1c-85a840fbe079",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98825a04-6dab-490a-9a6d-643ba5e8852a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This separates the problem of \"what to compute?\" from \"when/where/on what resources to compute it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723b4a9-56b3-4975-85eb-e3fa188925db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@dask.delayed\n",
    "def start():\n",
    "    print(\"start\")\n",
    "    time.sleep(0.5)\n",
    "    return 1\n",
    "\n",
    "@dask.delayed\n",
    "def concurrent(initial, i):\n",
    "    time.sleep(np.random.uniform(0, 1))\n",
    "    print(f\"concurrent {i}\", end=\"\")\n",
    "    time.sleep(np.random.uniform(0, 0.05))\n",
    "    print()\n",
    "    return initial + i**2\n",
    "\n",
    "@dask.delayed\n",
    "def combine(partial_results):\n",
    "    time.sleep(0.5)\n",
    "    print(\"combine\")\n",
    "    return sum(partial_results)\n",
    "\n",
    "initial = start()\n",
    "output = combine([concurrent(initial, i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecebaf-e33a-407b-994c-d19f222a7782",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c967af4-1bc2-4c28-8ab9-cb93bb9c4a17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Dask has three built-in schedulers (rarely used in production):\n",
    "\n",
    "* `\"synchronous\"`: not parallel, intended for debugging\n",
    "* `\"threads\"`: multiple threads in the same process, limited by the [Python GIL](https://realpython.com/python-gil/)\n",
    "* `\"processes\"`: multiple Python processes; not affected by the GIL, but it has to start a bunch of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecbb28-60f9-4012-95aa-cd31d76dadf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19814c21-82a4-45fb-b73e-59e0efaa0495",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set(scheduler=\"synchronous\"):\n",
    "    output.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b52d26-e02b-44f5-bb5f-6c220ef7ecff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In a real system, we're more likely to use the Distributed library (or a third-party, such as Ray)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c975f5-32c9-4c06-a65d-5da8415e0a4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"../img/distributed-overview.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d4a82-14be-4e6d-93cf-2ed013a39aa5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Run in separate terminals:\n",
    "\n",
    "```bash\n",
    "dask-scheduler\n",
    "```\n",
    "\n",
    "and several of the following:\n",
    "\n",
    "```bash\n",
    "dask-worker --nthreads 1 127.0.0.1:8786\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda5932-f3a0-4faf-b00a-ce3b1e7e485a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "\n",
    "client = dask.distributed.Client(\"127.0.0.1:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44ebc8-b2ce-447f-8e52-2e41a03fe219",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892f9f9-5f12-45db-bcab-032613a43d4d",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ccb0-2760-46d7-bbc6-305f6600a7d4",
   "metadata": {},
   "source": [
    "## Dask collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ecd81-a43e-4864-bdc0-225e5f874c0e",
   "metadata": {},
   "source": [
    "You can build general computations with `@dask.delayed`, but there are some common patterns that we'd want to build all the time.\n",
    "\n",
    "For instance, splitting a calculation on NumPy arrays into embarrassingly parallel parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b333384-56d8-46d1-bdfe-4170b927fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c835a1d-7fb7-4faf-a3c6-cbbf712d3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "dataset_hdf5 = h5py.File(os.getcwd() + \"/../data/SMHiggsToZZTo4L.h5\")\n",
    "\n",
    "pt1 = da.from_array(dataset_hdf5[\"ee_mumu\"][\"e1\"][\"pt\"], chunks=10000)\n",
    "phi1 = da.from_array(dataset_hdf5[\"ee_mumu\"][\"e1\"][\"phi\"], chunks=10000)\n",
    "eta1 = da.from_array(dataset_hdf5[\"ee_mumu\"][\"e1\"][\"eta\"], chunks=10000)\n",
    "pt2 = da.from_array(dataset_hdf5[\"ee_mumu\"][\"e2\"][\"pt\"], chunks=10000)\n",
    "phi2 = da.from_array(dataset_hdf5[\"ee_mumu\"][\"e2\"][\"phi\"], chunks=10000)\n",
    "eta2 = da.from_array(dataset_hdf5[\"ee_mumu\"][\"e2\"][\"eta\"], chunks=10000)\n",
    "\n",
    "pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9fc1a-bcee-4145-ac8b-8f90d7682b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = np.sqrt(2*pt1*pt2*(np.cosh(eta1 - eta2) - np.cos(phi1 - phi2)))\n",
    "mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66f801-b095-4328-af50-9b898781769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc665d-2ed9-4065-808b-8b48f42117b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hist import Hist\n",
    "\n",
    "Hist.new.Reg(120, 0, 120, name=\"dimuon mass\").Double().fill(\n",
    "    mass.compute()\n",
    ").plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9961f-a5e7-4811-9e37-0119c73a26eb",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ea2ec-a707-4b6a-9d48-a1cd4f6235f2",
   "metadata": {},
   "source": [
    "<img src=\"../img/dask-overview.svg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb704b9c-c33e-4bbd-b2b0-1c9930cf6b91",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5718c-8971-494e-a38a-f3ec95d70ded",
   "metadata": {},
   "source": [
    "## The dask-awkward collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd747f3-1044-4a6a-8743-b10dc052f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "events = uproot.dask(os.getcwd() + \"/../data/SMHiggsToZZTo4L.root\")\n",
    "\n",
    "# events = uproot.dask(\n",
    "#     \"https://pivarski-princeton.s3.amazonaws.com/cms-open-dimuons/Run2012B_DoubleMuParked.root\",\n",
    "#     step_size=1000,\n",
    "# )\n",
    "\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7b822-9b1e-4896-a86a-bf0e9c9b12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = events[ak.num(events.Electron_pt) == 2]\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc7eef-ece5-4376-952d-7885fb8d29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1 = selected.Electron_pt[:, 0]\n",
    "phi1 = selected.Electron_phi[:, 0]\n",
    "eta1 = selected.Electron_eta[:, 0]\n",
    "pt2 = selected.Electron_pt[:, 1]\n",
    "phi2 = selected.Electron_phi[:, 1]\n",
    "eta2 = selected.Electron_eta[:, 1]\n",
    "\n",
    "pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431f9ea-9d51-4ff6-be54-af97ce0a70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mass = np.sqrt(2*pt1*pt2*(np.cosh(eta1 - eta2) - np.cos(phi1 - phi2)))\n",
    "mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7a523-e484-4f81-8397-2aec3e21195a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mass.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40884fde-6c78-484e-8a1c-d929ab0461d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hist.new.Reg(120, 0, 120, name=\"dimuon mass\").Double().fill(\n",
    "    mass.compute()\n",
    ").plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9a26a-5e7b-4650-a0d4-cb638004b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist.dask as hda\n",
    "\n",
    "mass_dask_hist = hda.Hist.new.Reg(120, 0, 120, name=\"dimuon mass\").Double().fill(\n",
    "    mass\n",
    ")\n",
    "mass_dask_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358c4a9-fd1b-4b1e-bea9-840243254a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dask_hist.compute().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571bc51-cb45-4a65-9619-61e64592d6e6",
   "metadata": {},
   "source": [
    "## Building Distributed Objects in Dask\n",
    "\n",
    "An extremely common pattern in Dask is embarassingly parallel processing.\n",
    "i.e. Repeating the computation on different data that's being loaded in from a file in chunks.\n",
    "This could be on a single machine, or it could be multiple machines in a computing cluster.\n",
    "\n",
    "This becomes tricky when the operation itself becomes resource intensive in one way or another.\n",
    "Histograms actually serve as a great demonstration of this problem, especially given the modern\n",
    "tools we have (Hist, and similar) that let us produce many-dimensional histograms easily.\n",
    "\n",
    "Generating a billion bin histogram is easily realizable and when considering weighted filling this is \n",
    "a 16 GB histogram. Histograms this size have already been made for analyses, CMS W-mass ~8 GB, and \n",
    "50GB histograms are possible when dealing with large amounts of systematics.\n",
    "\n",
    "Given that these histograms do not fit on a single typical batch slot, what can we do to make full use of\n",
    "our modern software *without* requiring dedicated services or very custom code?\n",
    "\n",
    "Dask provides all the necessary primitives to build software that scales, and we'll take a look at that!\n",
    "\n",
    "\n",
    "The following is code that I'm developing towards a contribution to a histogramming library.\n",
    "I thought it might be useful to see how these kind of objects get made.\n",
    "\n",
    "This is going to use a number of the concepts we introduced on Tuesday, in addition to the things we discussed today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245f2a-c99c-4fec-99c7-d50b8066fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first stop the scheduler and worker in the terminal\n",
    "# then open a new browser tab at localhost:8787 to see\n",
    "# the dashboard\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981e26a-f035-4cc6-b52a-f6e59415be76",
   "metadata": {},
   "source": [
    "A core idea we're going to need here is the ability to partially reduce the data, instead of having one object being output that's *all* of the data represented in one array. We want multiple arrays that each contain partial sums of the data.\n",
    "\n",
    "This class below creates a dask task graph that does exactly this, in addition to applying the functions needed to create the sums and add them together correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e34ac-9878-4da2-807c-a064945493e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dask.layers import Layer\n",
    "\n",
    "from typing import Any, Callable\n",
    "\n",
    "import math\n",
    "import toolz\n",
    "import numpy as np\n",
    "\n",
    "class TruncatedTreeReduction(Layer):\n",
    "    \"\"\"Truncated Tree Reduction\n",
    "\n",
    "    This reduction stops when the the number of\n",
    "    partitions in a tree reduction is smaller than\n",
    "    split_every.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name to use for the constructed layer.\n",
    "    name_input : str\n",
    "        Name of the input layer that is being reduced.\n",
    "    npartitions_input : int\n",
    "        Number of partitions in the input layer.\n",
    "    concat_func : callable\n",
    "        Function used by each tree node to reduce a list of inputs\n",
    "        into a single output value. This function must accept only\n",
    "        a list as its first positional argument.\n",
    "    tree_node_func : callable\n",
    "        Function used on the output of ``concat_func`` in each tree\n",
    "        node. This function must accept the output of ``concat_func``\n",
    "        as its first positional argument.\n",
    "    finalize_func : callable, optional\n",
    "        Function used in place of ``tree_node_func`` on the final tree\n",
    "        node(s) to produce the final output for each split. By default,\n",
    "        ``tree_node_func`` will be used.\n",
    "    split_every : int, optional\n",
    "        This argument specifies the maximum number of input nodes\n",
    "        to be handled by any one task in the tree. Defaults to 32.\n",
    "    tree_node_name : str, optional\n",
    "        Name to use for intermediate tree-node tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    name_input: str\n",
    "    npartitions_input: int\n",
    "    npartitions_stop: int\n",
    "    concat_func: Callable\n",
    "    tree_node_func: Callable\n",
    "    finalize_func: Callable | None\n",
    "    split_every: int\n",
    "    nnewax_partitions: int | None\n",
    "    output_partitions: list[int] | list[tuple[int]]\n",
    "    tree_node_name: str\n",
    "    widths: list[int]\n",
    "    height: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        name_input: str,\n",
    "        npartitions_input: int, \n",
    "        npartitions_stop: int,\n",
    "        concat_func: Callable,\n",
    "        tree_node_func: Callable,\n",
    "        finalize_func: Callable | None = None,\n",
    "        split_every: int = 32,\n",
    "        nnewax_partitions: int | None = None,\n",
    "        newax_partition_size: int | None = None,\n",
    "        tree_node_name: str | None = None,\n",
    "        annotations: dict[str, Any] | None = None,\n",
    "    ):\n",
    "        super().__init__(annotations=annotations)\n",
    "\n",
    "        if nnewax_partitions is None != newax_partition_size is None:\n",
    "            raise ValueError(\"both new axis partitions and partition size must be defined!\")\n",
    "        \n",
    "        self.name = name\n",
    "        self.name_input = name_input\n",
    "        self.npartitions_input = npartitions_input\n",
    "        self.npartitions_stop = npartitions_stop\n",
    "        self.concat_func = concat_func\n",
    "        self.tree_node_func = tree_node_func\n",
    "        self.finalize_func = finalize_func\n",
    "        self.split_every = split_every\n",
    "        self.nnewax_partitions = nnewax_partitions\n",
    "        self.newax_partition_size = newax_partition_size\n",
    "        self.tree_node_name = tree_node_name or \"tree_node-\" + self.name\n",
    "\n",
    "        # Calculate tree widths and height\n",
    "        # (Used to get output keys without materializing)\n",
    "        parts = self.npartitions_input\n",
    "        self.widths = [parts]\n",
    "        while parts > max(self.split_every, self.npartitions_stop):\n",
    "            parts = math.ceil(parts / self.split_every)\n",
    "            self.widths.append(int(parts))\n",
    "        self.widths[-1] = self.npartitions_stop\n",
    "        self.height = len(self.widths)\n",
    "\n",
    "        npartitions = (\n",
    "            self.npartitions_stop if not self.nnewax_partitions\n",
    "            else self.npartitions_stop * self.nnewax_partitions\n",
    "        )\n",
    "        partitions_shape = (\n",
    "            (self.npartitions_stop, ) if not self.nnewax_partitions\n",
    "            else (self.npartitions_stop, self.nnewax_partitions)\n",
    "        )\n",
    "        \n",
    "        self.output_partitions = (\n",
    "            list(range(npartitions)) if not self.nnewax_partitions\n",
    "            else list( (int(i), int(j)) for i, j in zip(*np.unravel_index(np.arange(npartitions), partitions_shape)) )\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _make_key(self, *name_parts, split=None):\n",
    "        # Helper function construct a key\n",
    "        # with a \"split\" element when\n",
    "        # bool(split_out) is True\n",
    "        return name_parts + (split,) if split else name_parts\n",
    "\n",
    "    def _define_task(self, input_keys, split_and_size=None, final_task=False):\n",
    "        # Define nested concatenation and func task\n",
    "        if final_task and self.finalize_func: \n",
    "            outer_func = self.finalize_func\n",
    "            if split_and_size is not None:\n",
    "                split, size = split_and_size\n",
    "                outer_func = lambda x: self.finalize_func(x, split=split, size=size)\n",
    "        else:\n",
    "            outer_func = self.tree_node_func\n",
    "        return (toolz.pipe, input_keys, self.concat_func, outer_func)\n",
    "\n",
    "    def _construct_graph(self):\n",
    "        \"\"\"Construct graph for a tree reduction.\"\"\"\n",
    "\n",
    "        dsk = {}\n",
    "        if not self.output_partitions:\n",
    "            return dsk\n",
    "\n",
    "        if self.height >= 2:\n",
    "            # Loop over reduction levels\n",
    "            for depth in range(1, self.height):\n",
    "                # Loop over reduction groups\n",
    "                if depth == self.height - 1:\n",
    "                    split_every_base, overlap = divmod(self.widths[depth-1], self.widths[depth])\n",
    "                else:\n",
    "                    split_every_base, overlap = self.split_every, 0\n",
    "                for group in range(self.widths[depth]):\n",
    "                    # Calculate inputs for the current group\n",
    "                    split_every = split_every_base + 1 if group < overlap else split_every_base\n",
    "                    p_max = self.widths[depth - 1]\n",
    "                    lstart = split_every * group\n",
    "                    if group >= overlap:\n",
    "                        lstart += overlap\n",
    "                    lstop = min(lstart + split_every, p_max)\n",
    "                    if depth == 1:\n",
    "                        # Input nodes are from input layer\n",
    "                        input_keys = [\n",
    "                            self._make_key(self.name_input, p)\n",
    "                            for p in range(lstart, lstop)\n",
    "                        ]\n",
    "                    else:\n",
    "                        # Input nodes are tree-reduction nodes\n",
    "                        input_keys = [\n",
    "                            self._make_key(\n",
    "                                self.tree_node_name, p, depth - 1\n",
    "                            )\n",
    "                            for p in range(lstart, lstop)\n",
    "                        ]\n",
    "\n",
    "                    # Define task\n",
    "                    if depth == self.height - 1:\n",
    "                        # Final Node (Use fused `self.tree_finalize` task)\n",
    "                        if self.nnewax_partitions:\n",
    "                            for new_part in range(self.nnewax_partitions):\n",
    "                                dsk[(self.name, group, new_part)] = self._define_task(\n",
    "                                    input_keys, \n",
    "                                    split_and_size=(new_part, self.newax_partition_size), \n",
    "                                    final_task=True\n",
    "                                )\n",
    "                        else:\n",
    "                            dsk[(self.name, group)] = self._define_task(\n",
    "                                input_keys, final_task=True\n",
    "                            )\n",
    "                    else:\n",
    "                        # Intermediate Node\n",
    "                        dsk[\n",
    "                            self._make_key(\n",
    "                                self.tree_node_name, group, depth\n",
    "                            )\n",
    "                        ] = self._define_task(input_keys, final_task=False)\n",
    "        else:\n",
    "            # Deal with single-partition case\n",
    "            for s in self.output_partitions:\n",
    "                if isinstance(s, tuple):\n",
    "                    input_keys = [self._make_key(self.name_input, s[0])]\n",
    "                    dsk[(self.name, *s)] = self._define_task(\n",
    "                        input_keys, \n",
    "                        split_and_size=(s[1], self.newax_partition_size), \n",
    "                        final_task=True\n",
    "                    )\n",
    "                else:\n",
    "                    input_keys = [self._make_key(self.name_input, s)]\n",
    "                    dsk[(self.name, s)] = self._define_task(input_keys, final_task=True)\n",
    "\n",
    "        return dsk\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"DataFrameTreeReduction<name='{}', input_name={}, nnewax_partitions={}>\".format(\n",
    "            self.name, self.name_input, self.nnewax_partitions\n",
    "        )\n",
    "\n",
    "    def _output_keys(self):\n",
    "        return {(self.name, *s) if isinstance(s, tuple) else (self.name, s) for s in self.output_partitions}\n",
    "\n",
    "    def get_output_keys(self):\n",
    "        if hasattr(self, \"_cached_output_keys\"):\n",
    "            return self._cached_output_keys\n",
    "        else:\n",
    "            output_keys = self._output_keys()\n",
    "            self._cached_output_keys = output_keys\n",
    "        return self._cached_output_keys\n",
    "\n",
    "    def is_materialized(self):\n",
    "        return hasattr(self, \"_cached_dict\")\n",
    "\n",
    "    @property\n",
    "    def _dict(self):\n",
    "        \"\"\"Materialize full dict representation\"\"\"\n",
    "        if hasattr(self, \"_cached_dict\"):\n",
    "            return self._cached_dict\n",
    "        else:\n",
    "            dsk = self._construct_graph()\n",
    "            self._cached_dict = dsk\n",
    "        return self._cached_dict\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._dict[key]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Start with \"base\" tree-reduction size\n",
    "        tree_size = (sum(self.widths[1:]) or 1) * (self.nnewax_partitions or 1)\n",
    "        if self.nnewax_partitions:\n",
    "            # Add on \"split-*\" tasks used for `getitem` ops\n",
    "            return tree_size + len(self.output_partitions)\n",
    "        return tree_size\n",
    "\n",
    "    def _keys_to_output_partitions(self, keys):\n",
    "        \"\"\"Simple utility to convert keys to output partition indices.\"\"\"\n",
    "        splits = set()\n",
    "        for key in keys:\n",
    "            try:\n",
    "                _name, _split = key\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if _name != self.name:\n",
    "                continue\n",
    "            splits.add(_split)\n",
    "        return splits\n",
    "\n",
    "    def _cull(self, output_partitions):\n",
    "        return TruncatedTreeReduction(\n",
    "            self.name,\n",
    "            self.name_input,\n",
    "            self.npartitions_input,\n",
    "            self.npartitions_stop,\n",
    "            self.concat_func,\n",
    "            self.tree_node_func,\n",
    "            finalize_func=self.finalize_func,\n",
    "            split_every=self.split_every,\n",
    "            nnewax_partitions=self.nnewax_partitions,\n",
    "            newax_partition_size=self.newax_partition_size,\n",
    "            tree_node_name=self.tree_node_name,\n",
    "            annotations=self.annotations,\n",
    "        )\n",
    "\n",
    "    def cull(self, keys, all_keys):\n",
    "        \"\"\"Cull a DataFrameTreeReduction HighLevelGraph layer\"\"\"\n",
    "        deps = {\n",
    "            (self.name, 0): {\n",
    "                (self.name_input, i) for i in range(self.npartitions_input)\n",
    "            }\n",
    "        }\n",
    "        output_partitions = self._keys_to_output_partitions(keys)\n",
    "        if output_partitions != set(self.output_partitions):\n",
    "            culled_layer = self._cull(output_partitions)\n",
    "            return culled_layer, deps\n",
    "        else:\n",
    "            return self, deps\n",
    "\n",
    "    def mock(self) -> TruncatedTreeReduction:\n",
    "        return TruncatedTreeReduction(\n",
    "            name=self.name,\n",
    "            name_input=self.name_input,\n",
    "            npartitions_input=1,\n",
    "            concat_func=self.concat_func,\n",
    "            tree_node_func=self.tree_node_func,\n",
    "            finalize_func=self.finalize_func,\n",
    "            split_every=self.split_every,\n",
    "            nnewax_partitions=1 if self.nnewax_partitions else self.nnewax_partitions,\n",
    "            newax_partition_size=self.newax_partition_size,\n",
    "            tree_node_name=self.tree_node_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6028b01-9c91-4010-b0fe-4cb06ebd8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# make some data\n",
    "ndata = 125_000_000\n",
    "chunk_size = 125_000\n",
    "fill_weighted = True\n",
    "ndims = 3\n",
    "data = da.random.normal(size=(ndata, ndims), chunks=(chunk_size, ndims))\n",
    "weights = da.random.normal(loc=1, scale=0.05, size=(ndata, ), chunks=(chunk_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff377bd-fdfe-483f-b26b-24f6eed45c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some bins\n",
    "nbins = 50\n",
    "bins = da.linspace(-5, 5, nbins+1)\n",
    "\n",
    "nd_bins = ndims*[(-5, 5, nbins)]\n",
    "nd_nbins = math.prod(i_bin[2] for i_bin in nd_bins)\n",
    "nd_minlength = math.prod(i_bin[2]+2 for i_bin in nd_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed6bd3-7f62-45e3-8bfa-7e2f0b7f4064",
   "metadata": {},
   "source": [
    "## Building a histogramming strategy\n",
    "\n",
    "What we want is a histogram that never fully appears on any particular worker.\n",
    "This means that we can't create the \"dense\" histogram and fill that, so we must \n",
    "think towards how we can represent a filled histogram logically!\n",
    "\n",
    "- Given an N-dimensional histogram we can always figure out what bin a piece of data\n",
    "*should* be in.\n",
    "- We can figure out how many (weighted) entries are in a bin using np.unique!\n",
    "- Once we know how many entries are in bins in slices of our data we can add those slices, keeping track of the indices\n",
    "\n",
    "Since all of this is bookkeeping the indices, rather than passing around and filling a histogram object, we can avoid\n",
    "rendering the complete histogram on any particular worker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb3bbf-3877-4da8-8c8d-cb26d96fe27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything we'll need to build our distributed histogram\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from dask.array.core import Array as dArray\n",
    "from dask.blockwise import blockwise, BlockIndex\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "\n",
    "from functools import partial\n",
    "import numba as nb\n",
    "\n",
    "nb.config.THREADING_LAYER = 'threadsafe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a1fbf-f09f-46f2-b5d9-e1e16ce2f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dtype = np.dtype(\n",
    "    [(\"indices\", np.int64), (\"counts\", np.int64)]\n",
    ")\n",
    "\n",
    "weights_dtype = np.float64\n",
    "\n",
    "weighted_dtype = np.dtype(\n",
    "    [(\"indices\", np.int64), (\"sumw\", weights_dtype), (\"sumw2\", weights_dtype)]\n",
    ")\n",
    "\n",
    "def _noop(x):\n",
    "    return x\n",
    "\n",
    "# for this simple example bins are in (lo, hi, count) format\n",
    "# we can extend this later to include all the typical varieties of dense bins\n",
    "def bin_data(data, bins, **_):\n",
    "    to_bin = data if len(data.shape) > 1 else data[None]\n",
    "    shape = []\n",
    "    binned_data = []\n",
    "    for ax, (lo, hi, nbins_ax) in enumerate(bins):\n",
    "        bins_ax = np.linspace(lo, hi, nbins_ax + 1)\n",
    "        shape.append(nbins_ax + 2)\n",
    "        binned_data.append(np.searchsorted(bins_ax, to_bin[:, ax], side=\"right\"))\n",
    "    nd_bins = np.stack(binned_data, )\n",
    "    return np.ravel_multi_index(binned_data, tuple(shape))\n",
    "\n",
    "def cheap_sparse_sum(arrays):\n",
    "    out = arrays.pop()\n",
    "    locs = out[\"indices\"]\n",
    "    is_weighted = False\n",
    "    if \"counts\" in out.dtype.names:\n",
    "        counts = out[\"counts\"]\n",
    "    else:\n",
    "        is_weighted = True\n",
    "        sumw_w2 = np.stack((out[\"sumw\"], out[\"sumw2\"]), axis=1)\n",
    "        \n",
    "    while len(arrays):\n",
    "        to_add = arrays.pop()\n",
    "        ilocs = to_add[\"indices\"]\n",
    "        if is_weighted:\n",
    "            isumw_w2 = np.stack((to_add[\"sumw\"], to_add[\"sumw2\"]), axis=1)\n",
    "        else:\n",
    "            icounts = to_add[\"counts\"]            \n",
    "        catted_coords = np.r_[locs, ilocs]\n",
    "        nlocs, inverse = np.unique(catted_coords, return_inverse=True)\n",
    "        if is_weighted:\n",
    "            updated_sumw_w2 = np.zeros((nlocs.size, 2), dtype=sumw_w2.dtype)\n",
    "            updated_sumw_w2[inverse[:locs.size]] += sumw_w2\n",
    "            updated_sumw_w2[inverse[locs.size:]] += isumw_w2\n",
    "            sumw_w2 = updated_sumw_w2\n",
    "        else:\n",
    "            updated_counts = np.zeros_like(nlocs, dtype=counts.dtype)\n",
    "            updated_counts[inverse[:locs.size]] += counts\n",
    "            updated_counts[inverse[locs.size:]] += icounts\n",
    "            counts = updated_counts\n",
    "        locs = nlocs\n",
    "    out = np.empty(\n",
    "        len(locs), \n",
    "        dtype=(\n",
    "            weighted_dtype if is_weighted\n",
    "            else counts_dtype\n",
    "        )\n",
    "    )\n",
    "    out[\"indices\"] = locs\n",
    "    if is_weighted:\n",
    "        out[\"sumw\"], out[\"sumw2\"] = sumw_w2[:,0], sumw_w2[:, 1]\n",
    "    else:\n",
    "        out[\"counts\"] = counts\n",
    "    return out\n",
    "\n",
    "\n",
    "@nb.jit(\n",
    "    [\n",
    "        nb.void(nb.float32[:], nb.float32[:], nb.int64[:], nb.float32[:]), #float histogram, signed bins\n",
    "        nb.void(nb.float64[:], nb.float64[:], nb.int64[:], nb.float64[:]), #double histogram, signed bins\n",
    "        nb.void(nb.float32[:], nb.float32[:], nb.uint64[:], nb.float32[:]), #float histogram, unsigned bins\n",
    "        nb.void(nb.float64[:], nb.float64[:], nb.uint64[:], nb.float64[:]), #double histogram, unsigned bins\n",
    "    ],\n",
    "    nopython=True, \n",
    "    parallel=False, \n",
    "    nogil=True\n",
    ")\n",
    "def fill_sumw(sumw, sumw2, inverse, weights):\n",
    "    for pos in range(inverse.size):\n",
    "        idx = inverse[pos]\n",
    "        weight = weights[pos]\n",
    "        sumw[idx] += weight\n",
    "        sumw2[idx] += weight**2\n",
    "\n",
    "\n",
    "def sparse_bincount(bins, weights=None, **_):\n",
    "    if weights is None:\n",
    "        locs, counts = np.unique(bins, return_counts=True)\n",
    "\n",
    "        out = np.empty(locs.shape, dtype=[(\"indices\", locs.dtype), (\"counts\", counts.dtype)])\n",
    "        out[\"indices\"] = locs\n",
    "        out[\"counts\"] = counts\n",
    "        \n",
    "        return out\n",
    "\n",
    "    locs, inverse = np.unique(bins, return_inverse=True)\n",
    "    sumw, sumw2 = np.zeros(locs.shape, dtype=weights.dtype), np.zeros(locs.shape, dtype=weights.dtype)\n",
    "    fill_sumw(sumw, sumw2, inverse, weights)\n",
    "\n",
    "    out = np.empty(locs.size, dtype=[(\"indices\", locs.dtype), (\"sumw\", sumw.dtype), (\"sumw2\", sumw2.dtype)])\n",
    "    out[\"indices\"] = locs\n",
    "    out[\"sumw\"] = sumw\n",
    "    out[\"sumw2\"] = sumw2\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "@nb.jit(\n",
    "    [\n",
    "        nb.void(nb.int64[:], nb.int64[:], nb.int64[:], nb.int64, nb.int64), #integer histogram, signed bins\n",
    "        nb.void(nb.uint64[:], nb.int64[:], nb.uint64[:], nb.int64, nb.int64), #unsigned histogram, signed bins\n",
    "        nb.void(nb.float32[:], nb.int64[:], nb.float32[:], nb.int64, nb.int64), #float histogram, signed bins\n",
    "        nb.void(nb.float64[:], nb.int64[:], nb.float64[:], nb.int64, nb.int64), #double histogram, signed bins\n",
    "        nb.void(nb.int64[:], nb.uint64[:], nb.int64[:], nb.uint64, nb.uint64), #integer histogram, unsigned bins\n",
    "        nb.void(nb.uint64[:], nb.uint64[:], nb.uint64[:], nb.uint64, nb.uint64), #unsigned histogram, unsigned bins\n",
    "        nb.void(nb.float32[:], nb.uint64[:], nb.float32[:], nb.uint64, nb.uint64), #float histogram, unsigned bins\n",
    "        nb.void(nb.float64[:], nb.uint64[:], nb.float64[:], nb.uint64, nb.uint64), #double histogram, unsigned bins\n",
    "    ],\n",
    "    nopython=True, \n",
    "    parallel=False, \n",
    "    nogil=True,\n",
    ")\n",
    "def fill_slice(out, idxs, data, step, size):\n",
    "    for pos in range(idxs.size):\n",
    "        idx = idxs[pos]\n",
    "        if (idx < step + size) and (idx >= step):\n",
    "            out[idx - step] += data[pos]\n",
    "\n",
    "@nb.jit(\n",
    "    [\n",
    "        nb.void(nb.float32[:], nb.float32[:], nb.int64[:], nb.float32[:], nb.float32[:], nb.int64, nb.int64), #float histogram, signed bins\n",
    "        nb.void(nb.float64[:], nb.float64[:], nb.int64[:], nb.float64[:], nb.float64[:], nb.int64, nb.int64), #double histogram, signed bins\n",
    "        nb.void(nb.float32[:], nb.float32[:], nb.uint64[:], nb.float32[:], nb.float32[:], nb.uint64, nb.uint64), #float histogram, unsigned bins\n",
    "        nb.void(nb.float64[:], nb.float64[:], nb.uint64[:], nb.float64[:], nb.float64[:], nb.uint64, nb.uint64), #double histogram, unsigned bins\n",
    "    ],\n",
    "    nopython=True, \n",
    "    parallel=False, \n",
    "    nogil=True,\n",
    ")\n",
    "def fill_slice_weighted(sumw_out, sumw2_out, idxs, sumw, sumw2, step, size):\n",
    "    for pos in range(idxs.size):\n",
    "        idx = idxs[pos]\n",
    "        if (idx < step + size) and (idx >= step):\n",
    "            sumw_out[idx - step] += sumw[pos]\n",
    "            sumw2_out[idx - step] += sumw2[pos]\n",
    "\n",
    "\n",
    "def sparse_to_dense_chunked_bincount(bincounts, index, size, maxbin):\n",
    "    nb.config.THREADING_LAYER = 'threadsafe'\n",
    "    step = index[1]*size\n",
    "    step_up = min(step + size, maxbin)\n",
    "    is_weighted = (\"sumw\" in bincounts.dtype.names)\n",
    "    dense_out = (\n",
    "        np.zeros((step_up - step, 2), dtype=bincounts[\"sumw\"].dtype) if is_weighted\n",
    "        else np.zeros(step_up - step, dtype=np.int64)\n",
    "    )\n",
    "    if isinstance(bincounts, list):\n",
    "        out = bincounts.pop()\n",
    "        fill_slice(dense_out, out.coords.squeeze(), out.data, step, size)\n",
    "        while len(bincounts):\n",
    "            out = bincounts.pop()\n",
    "            fill_slice(dense_out, out.coords.squeeze(), out.data, step, size)\n",
    "        return dense_out\n",
    "    #idx = bincounts.coords.squeeze()\n",
    "    #mask = (idx < step + size) & (idx >= step)\n",
    "    #dense_out[idx[mask] - step] += bincounts.data[mask]\n",
    "    if is_weighted:\n",
    "        fill_slice_weighted(\n",
    "            dense_out[:,0], dense_out[:,1], \n",
    "            bincounts[\"indices\"], bincounts[\"sumw\"], bincounts[\"sumw2\"], \n",
    "            step, size\n",
    "        )\n",
    "    else:\n",
    "        fill_slice(dense_out, bincounts[\"indices\"], bincounts[\"counts\"], step, size)\n",
    "    return dense_out[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44b4da-14bb-4eb5-8d44-eec28ad276c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest part: bin the data\n",
    "binned_sorted = data.map_blocks(bin_data, nd_bins, drop_axis=[1], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500dba0-0a77-452c-982e-6f37280e2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the binned data into sparse bin counts\n",
    "bincounts = binned_sorted.map_blocks(\n",
    "    sparse_bincount,\n",
    "    weights if fill_weighted else None,\n",
    "    chunks=(np.nan,),\n",
    "    dtype=weighted_dtype if fill_weighted else counts_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06298201-3718-41c1-bba7-40413c8e6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we create the on-cluster partitioned histogram\n",
    "name = \"dense_chunked_bincount\"\n",
    "hist_partitions = 10\n",
    "n_hist_slices = 25 # number of chunks to split input partitions into\n",
    "split_every = 8\n",
    "hist_partition_size = (nd_minlength)//hist_partitions + 1\n",
    "\n",
    "red_name = \"sparse_reduced_bincount\"\n",
    "sparse_hist_reduction_layer = TruncatedTreeReduction(\n",
    "    name=red_name,\n",
    "    name_input=bincounts.name,\n",
    "    npartitions_input=bincounts.npartitions,\n",
    "    npartitions_stop=n_hist_slices,\n",
    "    concat_func=cheap_sparse_sum,\n",
    "    tree_node_func=_noop,\n",
    "    #finalize_func=sparse_to_dense_chunked_bincount,\n",
    "    split_every=split_every,\n",
    "    #nnewax_partitions=hist_partitions,\n",
    "    #newax_partition_size=hist_partition_size,\n",
    ")\n",
    "\n",
    "red_output_parts = len(sparse_hist_reduction_layer.get_output_keys())\n",
    "red_hlg = HighLevelGraph.from_collections(red_name, sparse_hist_reduction_layer, dependencies=(bincounts, ))\n",
    "red_hist = dArray(\n",
    "    red_hlg, \n",
    "    red_name,\n",
    "    chunks=[red_output_parts*(np.nan, )],\n",
    "    dtype=weighted_dtype if fill_weighted else counts_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be0a75-506d-4157-8b2a-f4aa51e016e0",
   "metadata": {},
   "source": [
    "The final custom step is to take each partition of summed bin counts and turn that into *slices* of the dense histogram and then sum those slices together.\n",
    "\n",
    "Here we use a piece of dask called \"blockwise\" which allows us to define exactly how to apply a function across the partitions of an input array.\n",
    "It even allows us to make new axes and provides facilites for knowing which partition of data you're working on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056b904-bdb0-4f34-b854-4e8e802745c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_layer = blockwise(\n",
    "    sparse_to_dense_chunked_bincount, \n",
    "    name, \"ikj\" if fill_weighted else \"ik\", \n",
    "    red_hist.name, \"i\",\n",
    "    BlockIndex((red_output_parts, hist_partitions)), \"ik\",\n",
    "    hist_partition_size, None,\n",
    "    nd_minlength, None,\n",
    "    numblocks = {red_hist.name: (red_output_parts,)},\n",
    "    new_axes = (\n",
    "        {\"k\": tuple(hist_partitions*[hist_partition_size]), \"j\": tuple(hist_partitions*[2])} if fill_weighted\n",
    "        else {\"k\": tuple(hist_partitions*[hist_partition_size])}\n",
    "    ),\n",
    ")\n",
    "\n",
    "dist_hist_hlg = HighLevelGraph.from_collections(name, shuffle_layer, dependencies=(red_hist, ))\n",
    "dist_hist_sliced = dArray(\n",
    "    dist_hist_hlg, \n",
    "    name, \n",
    "    chunks=(\n",
    "        (1, hist_partition_size, 2) if fill_weighted\n",
    "        else (1, hist_partition_size)\n",
    "    ),\n",
    "    shape=(\n",
    "        (red_hist.npartitions, nd_minlength, 2) if fill_weighted\n",
    "        else (red_hist.npartitions, nd_minlength)\n",
    "    ),        \n",
    "    dtype=(\n",
    "        weights.dtype if fill_weighted\n",
    "        else np.int64\n",
    "    ),\n",
    ")\n",
    "\n",
    "dist_hist = dist_hist_sliced.sum(axis=0)\n",
    "\n",
    "# check it against da.histogramdd\n",
    "check_hist, _ = da.histogramdd(data, weights=weights if fill_weighted else None, bins=ndims*(nbins,), range=ndims*[(-5,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e5654-dfbb-45c7-990f-0e4f674a619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_hist.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9969d6-987e-41f5-83ed-95f24d7081a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check = check_hist.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3de2f-f134-41fc-852b-fcf6ef9db1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_hist = dist_hist.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218bb84-d897-4c7b-a075-846d29ffbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_weighted:\n",
    "    outcome = np.all(np.isclose(new_hist.reshape(52,52,52,2)[1:51, 1:51, 1:51, 0], check))\n",
    "else:\n",
    "    outcome = np.all(new_hist.reshape(52,52,52)[1:51, 1:51, 1:51] == check)\n",
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa3b36-113e-4d81-ac98-4e9929e1816d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
